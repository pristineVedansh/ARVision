{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“A learning model that summarizes data with a **set of parameters** of fixed size(independent of the number of training examples) is called a parametric model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs.” – Russell and Norvig (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simply* stating, **parametrized classification algorithms** learns patterns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Components of Parameterized Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameterization** is the process of defining the necessary parameters of a given model. In the task of machine learning, parameterization involves defining a problem in terms of four key components: *data*, *a scoring function*, *a loss function*, and *weights and biases*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Data\n",
    "This includes both data points(i.e., raw pixel intensities, features etc.) and the labels associated with each data points. We denote data in terms of **design matrix**. Each **row** in design matrix represents a **data point** and each **column** of the matrix corressponds to a different **features**. We also define a vector y where $y_{i}$ provides the class label of $i^{th}$ image in each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Function\n",
    "INPUT_IMAGES $\\rightarrow$ F(INPUT_IMAGES) $\\rightarrow$ OUTPUT_CLASS_LABELS.                                       Here **F** is a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "A loss function quantifies how well our *predicted class labels* agree with our *ground-truth labels*. The higher level of agreement between these two sets of labels, the lower our loss (and higher our classification accuracy, at least on the training set).\n",
    "Simply, loss function tells **how good our accuracy is**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and biases\n",
    "The weight matrix, typically denoted as $W$ and the bias vector $b$ are called the **weights** or\n",
    "parameters of our classifier that we’ll actually be optimizing. Based on the output of our scoring\n",
    "function and loss function, we’ll be tweaking and fiddling with the values of the weights and biases\n",
    "to increase classification accuracy. Our main aim is to **reduce our loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with data. \n",
    "**Training set** - $(x_i, y_i)$, $i = 1, ..., N$. $x_i$ is each data point and $y_i$ is the label associated with $x_i$. $N$ is the number of data points of dimensionality $D$.\n",
    "\n",
    "$y_i = 1, ..., K$. In words, $K$ is the number of output categories. For example, in case of dog and cat datset, we have $K=2$.\n",
    "\n",
    "Now, we must define **scoring function** $f$ that maps the image to the class label scores. One method to accomplish this is via linear mapping.\n",
    "\n",
    "$$f(x_i, W, b)=Wx_i+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Parameterized Learning\n",
    "- Once we are done with training our model, we can discard $x_i$ and **keep only the weight matrix $W$ and bias $b$**. This reduces the size of our model.\n",
    "- Classifying new test data is fast. In order to perform classification, all we need to do is take dot product of $W$ and $x_i$, followed by adding in the bias $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean by actually \"learning\"?\n",
    "In order to actually \"learn\" the mapping from input data to the labels via our scoring function, we need to discuss two important concepts:\n",
    "- Loss Functions [76], [77], [78], [79]\n",
    "- Optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loss Functions\n",
    "A loss function quantifies how \"good\" and \"bad\" a given predictor is at classifying the input data points in a dataset. \n",
    "Ideally, our loss should decrease over time as we tune our model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class SVM Loss\n",
    "Inspired by SVMs which uses a scoring function $f$ to map our **data points** to **numerical scores for each class levels**.\n",
    "\n",
    "$$f(x_i, W, b) = Wx_i+b$$\n",
    "Now, we have our scoring function, we need to determine how \"good\" or \"bad\" this function is(given $W$ and $b$) at making predictions. To make this determination, we need a **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access both $x_i$ as well as its associated label $y_i$.\n",
    "Let's say our scoring function is $$s=f(x_i, W)$$ which implies, we can obtain the predicted score of the j-th class(**y**) via the i-th data point:\n",
    "\n",
    "$$s_j=f(x_i,W)_j$$\n",
    "\n",
    "Using this syntax, we can obtain **hinge loss function**:\n",
    "\n",
    "$$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{{y}_i}+1)$$\n",
    "\n",
    "$s_j-s_{{y}_i}$ is **output label - actual label**\n",
    "\n",
    "**max** operation is clamping values at 0 to avoid inclusion of negative values.\n",
    "\n",
    "To derive loss across entire training set, we take mean over each $L_i$.\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n",
    "\n",
    "Another related loss function is the **square hinge loss**.\n",
    "\n",
    "$$L_i=\\sum_{j\\neq y_i}max{(0,s_j-s_{{y}_i}+1)}^2$$\n",
    "\n",
    "Square term penalizes loss which is not good. Now chosing between hinge and square hinge is just a hyperparameter we need to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Multi-Class SVM loss example\n",
    "Given are some arbitrary output scores of $f(x, W) = Wx+b$.\n",
    "<img src=\"files/6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see for the Image #1:\n",
    "We are seeing two terms here. First is $y_{cat} - y_{actual}$ and Second is $y_{panda}-y_{actual}$.\n",
    "We can see that error comes out to be 0, which shows that there is no error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(0, 1.33 - 4.26 + 1) + max(0, -1.01 - 4.26 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image #2 : Here our error comes out to be 5.96 which is clear by our wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.96"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(0, 3.76 - (-1.20) + 1) + max(0, -3.81 - (-1.20) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.199999999999999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(0, -2.37 - (-2.27) + 1) + max(0, 1.03 - (-2.27) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0 + 5.96 + 5.2) / 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our loss was **zero** for only one which implies two of our predictions were incorrect: which is true by above image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss and Softmax Classifiers\n",
    "Softmax classifiers give probabilities for each class label while hinge loss gives the margin.\n",
    "\n",
    "Mapping function is defined in the same way as hinge loss: $f(x_i, W)=Wx_i$\n",
    "\n",
    "Unlike hinge loss, we interpret these scores as **unnormalized log probabilities** for each class level, which amounts to swapping out the hinge loss function with cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_{i}=-log(\\frac{e^{s_{{Y}_i}}}{\\sum_{j}{e^{s_{j}})$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
