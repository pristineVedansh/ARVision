{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“A learning model that summarizes data with a **set of parameters** of fixed size(independent of the number of training examples) is called a parametric model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs.” – Russell and Norvig (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simply* stating, **parametrized classification algorithms** learns patterns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Components of Parameterized Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameterization** is the process of defining the necessary parameters of a given model. In the task of machine learning, parameterization involves defining a problem in terms of four key components: *data*, *a scoring function*, *a loss function*, and *weights and biases*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Data\n",
    "This includes both data points(i.e., raw pixel intensities, features etc.) and the labels associated with each data points. We denote data in terms of **design matrix**. Each **row** in design matrix($X_{i}$) represents a **data point** and each **column** of the matrix corressponds to a different **features**. We also define a vector y where $y_{i}$ provides the class label of $i^{th}$ image in each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Function\n",
    "INPUT_IMAGES $\\rightarrow$ F(INPUT_IMAGES) $\\rightarrow$ OUTPUT_CLASS_LABELS.                                       Here **F** is a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "A loss function quantifies how well our *predicted class labels* agree with our *ground-truth labels*. The higher level of agreement between these two sets of labels, the lower our loss (and higher our classification accuracy, at least on the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and biases\n",
    "The weight matrix, typically denoted as $W$ and the bias vector $b$ are called the **weights** or\n",
    "parameters of our classifier that we’ll actually be optimizing. Based on the output of our scoring\n",
    "function and loss function, we’ll be tweaking and fiddling with the values of the weights and biases\n",
    "to increase classification accuracy. Our main aim is to **reduce our loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
